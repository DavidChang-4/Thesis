{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# Imports\n",
    "###############################\n",
    "import torch\n",
    "\n",
    "# Tensordict modules\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import multiprocessing\n",
    "from tensordict import TensorDictBase\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "\n",
    "\n",
    "# Data collection\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "\n",
    "# Env\n",
    "from torchrl.envs import RewardSum, TransformedEnv\n",
    "from torchrl.envs.libs.vmas import VmasEnv\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "\n",
    "# Multi-agent network\n",
    "from torchrl.modules import MultiAgentMLP, ProbabilisticActor, TanhNormal\n",
    "\n",
    "# Loss\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "\n",
    "# Utils\n",
    "torch.manual_seed(0)\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import tempfile\n",
    "\n",
    "# CPU/GPU\n",
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (torch.device(0) if torch.cuda.is_available() and not is_fork else torch.device(\"cpu\"))\n",
    "vmas_device = device  # The device where the simulator is run (VMAS can run on GPU)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Hyperparams\n",
    "###############################\n",
    "\n",
    "# Sampling (frames per batch)\n",
    "frames_per_batch = 6_000  # Number of team frames collected per training iteration\n",
    "n_iters = 10  # Number of sampling and training iterations\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "# Training (batch size)\n",
    "num_epochs = 30  # Number of optimization steps per training iteration\n",
    "minibatch_size = 400  # Size of the mini-batches in each optimization step\n",
    "lr = 3e-4  # Learning rate\n",
    "max_grad_norm = 1.0  # Maximum norm for the gradients\n",
    "\n",
    "# PPO\n",
    "clip_epsilon = 0.2  # clip value for PPO loss\n",
    "gamma = 0.99  # discount factor\n",
    "lmbda = 0.9  # lambda for generalised advantage estimation\n",
    "entropy_eps = 1e-4  # coefficient of the entropy term in the PPO loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\vmas\\simulator\\utils.py:317: UserWarning: Scenario kwargs: {'render_mode': 'human'} passed but not used by the scenario. This will turn into an error in future versions.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# Environment Setup\n",
    "###############################\n",
    "max_steps = 100  # Steps before 'done' is automatically set.\n",
    "num_vmas_envs = (frames_per_batch // max_steps)  # Leverage batch simulation. \n",
    "scenario_name = \"simple_tag\"\n",
    "\n",
    "# Num pursuers/evaders\n",
    "n_pursuers = 2\n",
    "n_evaders = 1\n",
    "n_obstacles = 2\n",
    "render = \"human\" # '' or 'human'\n",
    "\n",
    "base_env = VmasEnv(\n",
    "    scenario=scenario_name,\n",
    "    num_envs=num_vmas_envs,\n",
    "    continuous_actions=True,\n",
    "    max_steps=max_steps,\n",
    "    device=vmas_device,\n",
    "\n",
    "    num_good_agents=n_evaders,\n",
    "    num_adversaries=n_pursuers,\n",
    "    num_landmarks=n_obstacles,\n",
    "    render_mode=render,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward_keys: [('adversary', 'reward'), ('agent', 'reward')]\n",
      "BoundedContinuous(\n",
      "    shape=torch.Size([60, 2, 2]),\n",
      "    space=ContinuousBox(\n",
      "        low=Tensor(shape=torch.Size([60, 2, 2]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
      "        high=Tensor(shape=torch.Size([60, 2, 2]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
      "    device=cuda:0,\n",
      "    dtype=torch.float32,\n",
      "    domain=continuous)\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# Printing environment keys\n",
    "###############################\n",
    "# All specs have leading shape (num_vmas_envs, n_agents) except for 'done.'\n",
    "# print(\"action_spec:\", env.full_action_spec)\n",
    "# print(\"reward_spec:\", env.full_reward_spec)\n",
    "# print(\"done_spec:\", env.full_done_spec)\n",
    "# print(\"observation_spec:\", env.observation_spec)\n",
    "\n",
    "# print(\"action_keys:\", env.action_keys)\n",
    "print(\"reward_keys:\", env.reward_keys)\n",
    "# print(\"done_keys:\", env.done_keys)\n",
    "print(base_env.full_action_spec[('adversary', 'action')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 04:47:49,513 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# Modify environment\n",
    "# Group pursuer rewards\n",
    "###############################\n",
    "env = TransformedEnv(\n",
    "    base_env,\n",
    "    RewardSum(\n",
    "        in_keys=base_env.reward_keys,\n",
    "        reset_keys=[\"_reset\"] * len(base_env.group_map.keys()),\n",
    "    )\n",
    ")\n",
    "check_env_specs(env) # Check env setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###############################\n",
    "# # Rollout\n",
    "# ############################### \n",
    "# with torch.no_grad():\n",
    "#    env.rollout(\n",
    "#        max_steps=max_steps,\n",
    "#        #policy=policy,\n",
    "#        callback=lambda env, _: env.render(),\n",
    "#        auto_cast_to_device=True,\n",
    "#        break_when_any_done=False,\n",
    "#    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Policy Network\n",
    "# Returns policy_modules[group], policies[group]\n",
    "###############################\n",
    "\n",
    "# 1) Create MLP agents for adversaries\n",
    "policy_modules = {}\n",
    "for group, agents in env.group_map.items():\n",
    "    share_parameters_policy = True\n",
    "\n",
    "    policy_net = torch.nn.Sequential(\n",
    "        MultiAgentMLP(\n",
    "            n_agent_inputs=env.observation_spec[group, \"observation\"].shape[-1],  # n_obs_per_agent\n",
    "            n_agent_outputs=2 * env.full_action_spec[group, \"action\"].shape[-1],  # 2 * n_actions_per_agents\n",
    "            n_agents=len(agents),\n",
    "            centralised=False,  # the policies are decentralised (ie each agent will act from its observation)\n",
    "            share_params=share_parameters_policy,\n",
    "            device=device,\n",
    "            depth=2,\n",
    "            num_cells=256,\n",
    "            activation_class=torch.nn.Tanh,\n",
    "        ), #NormalParamExtractor is REQUIRED\n",
    "        NormalParamExtractor(),  # this will just separate the last dimension into two outputs: a loc and a non-negative scale\n",
    "    )\n",
    "\n",
    "    # 2) Wrap NN -> TensorDictModule\n",
    "    policy_module = TensorDictModule(\n",
    "        policy_net,\n",
    "        in_keys=[(group, \"observation\")],\n",
    "        out_keys=[(group, \"loc\"), (group, \"scale\")], # TODO: maybe outkeys needs to be 1 param\n",
    "    )\n",
    "\n",
    "    policy_modules[group] = policy_module # Group = 'adversary' or 'agent'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3) Create ProbabilisticActor from policy networks\n",
    "policies = {}\n",
    "for group, _agents in env.group_map.items():\n",
    "    policy = ProbabilisticActor(\n",
    "        module=policy_modules[group],\n",
    "        spec=env.full_action_spec[group, \"action\"],\n",
    "        in_keys=[(group, \"loc\"), (group, \"scale\")],\n",
    "        out_keys=[(group, \"action\")],\n",
    "        distribution_class=TanhNormal,\n",
    "        distribution_kwargs={\n",
    "            \"low\": env.full_action_spec[group, \"action\"].space.low,\n",
    "            \"high\": env.full_action_spec[group, \"action\"].space.high,\n",
    "        },\n",
    "        return_log_prob=True,\n",
    "        log_prob_key=(group, \"sample_log_prob\"),\n",
    "    )  # we'll need the log-prob for the PPO loss\n",
    "    policies[group] = policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Critic Network\n",
    "# Returns critics[group]\n",
    "###############################\n",
    "share_parameters_critic = True\n",
    "mappo = True  # IPPO if False\n",
    "\n",
    "critics = {}\n",
    "for group, agents in env.group_map.items():\n",
    "\n",
    "    cat_module = TensorDictModule(\n",
    "        lambda obs, action: torch.cat([obs, action], dim=-1),\n",
    "        in_keys=[(group, \"observation\"), (group, \"action\")],\n",
    "        out_keys=[(group, \"obs_action\")],\n",
    "    )\n",
    "\n",
    "    # TODO: do i need cat module\n",
    "    critic_net = MultiAgentMLP(\n",
    "        n_agent_inputs=env.observation_spec[group, \"observation\"].shape[-1] + env.full_action_spec[group, \"action\"].shape[-1],\n",
    "        n_agent_outputs=1,  # 1 value per agent\n",
    "        n_agents=len(agents),\n",
    "        centralised=mappo,\n",
    "        share_params=share_parameters_critic,\n",
    "        device=device,\n",
    "        depth=2,\n",
    "        num_cells=256,\n",
    "        activation_class=torch.nn.Tanh,\n",
    "    )\n",
    "\n",
    "    critic_module = TensorDictModule(\n",
    "        module=critic_net,\n",
    "        in_keys=[(group, \"obs_action\")],\n",
    "        out_keys=[(group, \"state_value\")],\n",
    "    )\n",
    "\n",
    "    critics[group] = TensorDictSequential(\n",
    "        cat_module, critic_module\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running value and policy for group 'adversary': TensorDict(\n",
      "    fields={\n",
      "        adversary: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([60, 2, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                episode_reward: Tensor(shape=torch.Size([60, 2, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                loc: Tensor(shape=torch.Size([60, 2, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                obs_action: Tensor(shape=torch.Size([60, 2, 16]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([60, 2, 14]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                sample_log_prob: Tensor(shape=torch.Size([60, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                scale: Tensor(shape=torch.Size([60, 2, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                state_value: Tensor(shape=torch.Size([60, 2, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "            batch_size=torch.Size([60, 2]),\n",
      "            device=cuda:0,\n",
      "            is_shared=True),\n",
      "        agent: TensorDict(\n",
      "            fields={\n",
      "                episode_reward: Tensor(shape=torch.Size([60, 1, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([60, 1, 12]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "            batch_size=torch.Size([60, 1]),\n",
      "            device=cuda:0,\n",
      "            is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([60, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([60, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([60]),\n",
      "    device=cuda:0,\n",
      "    is_shared=True)\n",
      "Running value and policy for group 'agent': TensorDict(\n",
      "    fields={\n",
      "        adversary: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([60, 2, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                episode_reward: Tensor(shape=torch.Size([60, 2, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                loc: Tensor(shape=torch.Size([60, 2, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                obs_action: Tensor(shape=torch.Size([60, 2, 16]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([60, 2, 14]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                sample_log_prob: Tensor(shape=torch.Size([60, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                scale: Tensor(shape=torch.Size([60, 2, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                state_value: Tensor(shape=torch.Size([60, 2, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "            batch_size=torch.Size([60, 2]),\n",
      "            device=cuda:0,\n",
      "            is_shared=True),\n",
      "        agent: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([60, 1, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                episode_reward: Tensor(shape=torch.Size([60, 1, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                loc: Tensor(shape=torch.Size([60, 1, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                obs_action: Tensor(shape=torch.Size([60, 1, 14]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([60, 1, 12]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                sample_log_prob: Tensor(shape=torch.Size([60, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                scale: Tensor(shape=torch.Size([60, 1, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                state_value: Tensor(shape=torch.Size([60, 1, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "            batch_size=torch.Size([60, 1]),\n",
      "            device=cuda:0,\n",
      "            is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([60, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([60, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([60]),\n",
      "    device=cuda:0,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "# Run policy example\n",
    "reset_td = env.reset()\n",
    "for group, _agents in env.group_map.items():\n",
    "    print(\n",
    "        f\"Running value and policy for group '{group}':\",\n",
    "        critics[group](policies[group](reset_td)),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProbabilisticActor(\n",
      "    module=ModuleList(\n",
      "      (0): TensorDictModule(\n",
      "          module=Sequential(\n",
      "            (0): MultiAgentMLP(\n",
      "                MLP(\n",
      "                  (0): Linear(in_features=14, out_features=256, bias=True)\n",
      "                  (1): Tanh()\n",
      "                  (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (3): Tanh()\n",
      "                  (4): Linear(in_features=256, out_features=4, bias=True)\n",
      "                ),\n",
      "                n_agents=2,\n",
      "                share_params=True,\n",
      "                centralized=False,\n",
      "                agent_dim=-2)\n",
      "            (1): NormalParamExtractor(\n",
      "              (scale_mapping): biased_softplus()\n",
      "            )\n",
      "          ),\n",
      "          device=cuda:0,\n",
      "          in_keys=[('adversary', 'observation')],\n",
      "          out_keys=[('adversary', 'loc'), ('adversary', 'scale')])\n",
      "      (1): SafeProbabilisticModule()\n",
      "    ),\n",
      "    device=cuda:0,\n",
      "    in_keys=[('adversary', 'observation')],\n",
      "    out_keys=[('adversary', 'loc'), ('adversary', 'scale'), ('adversary', 'action'), ('adversary', 'sample_log_prob')])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\torchrl\\collectors\\collectors.py:194: UserWarning: A policy device was provided but no parameter/buffer could be found in the policy. Casting to policy_device is therefore impossible. The collector will trust that the devices match. To suppress this warning, set `trust_policy=True` when building the collector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1018], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m###############################\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Data Collector\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m###############################\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(policies[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madversary\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 5\u001b[0m collector \u001b[38;5;241m=\u001b[39m \u001b[43mSyncDataCollector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTensorDictSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpolicies\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madversary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicies\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43magent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmas_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstoring_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframes_per_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframes_per_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(collector)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m###############################\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Replay Buffer\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m###############################\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\torchrl\\collectors\\collectors.py:741\u001b[0m, in \u001b[0;36mSyncDataCollector.__init__\u001b[1;34m(self, create_env_fn, policy, frames_per_batch, total_frames, device, storing_device, policy_device, env_device, create_env_kwargs, max_frames_per_traj, init_random_frames, reset_at_each_iter, postproc, split_trajs, exploration_type, return_same_td, reset_when_done, interruptor, set_truncated, use_buffers, replay_buffer, trust_policy, compile_policy, cudagraph_policy, **kwargs)\u001b[0m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_shuttle()\n\u001b[0;32m    740\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_buffers:\n\u001b[1;32m--> 741\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_final_rollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_truncated_keys()\n\u001b[0;32m    744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_trajs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\torchrl\\collectors\\collectors.py:835\u001b[0m, in \u001b[0;36mSyncDataCollector._make_final_rollout\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m policy_input_copy \u001b[38;5;241m=\u001b[39m policy_input\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    832\u001b[0m policy_input_clone \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    833\u001b[0m     policy_input\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m    834\u001b[0m )  \u001b[38;5;66;03m# to test if values have changed in-place\u001b[39;00m\n\u001b[1;32m--> 835\u001b[0m policy_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;66;03m# check that we don't have exclusive keys, because they don't appear in keys\u001b[39;00m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_exclusive\u001b[39m(val):\n",
      "File \u001b[1;32mc:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\tensordict\\nn\\common.py:314\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m out\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _self \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(tensordict, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\tensordict\\nn\\utils.py:359\u001b[0m, in \u001b[0;36m_set_skip_existing_None.__call__.<locals>.wrapper\u001b[1;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev \u001b[38;5;241m=\u001b[39m _SKIP_EXISTING\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 359\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    361\u001b[0m     _SKIP_EXISTING \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev\n",
      "File \u001b[1;32mc:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\tensordict\\nn\\sequence.py:481\u001b[0m, in \u001b[0;36mTensorDictSequential.forward\u001b[1;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m         tensordict_exec \u001b[38;5;241m=\u001b[39m tensordict_exec\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule:\n\u001b[1;32m--> 481\u001b[0m         tensordict_exec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict_exec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorDictSequential does not support keyword arguments other than \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensordict_out\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or in_keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\tensordict\\nn\\sequence.py:456\u001b[0m, in \u001b[0;36mTensorDictSequential._run_module\u001b[1;34m(self, module, tensordict, **kwargs)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_module\u001b[39m(\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    449\u001b[0m     module: TensorDictModuleBase,\n\u001b[0;32m    450\u001b[0m     tensordict: TensorDictBase,\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    452\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_tolerant \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m    454\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m tensordict\u001b[38;5;241m.\u001b[39mkeys(include_nested\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39min_keys\n\u001b[0;32m    455\u001b[0m     ):\n\u001b[1;32m--> 456\u001b[0m         tensordict \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_tolerant \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensordict, LazyStackedTensorDict):\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sub_td \u001b[38;5;129;01min\u001b[39;00m tensordict\u001b[38;5;241m.\u001b[39mtensordicts:\n",
      "File \u001b[1;32mc:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\tensordict\\nn\\common.py:1244\u001b[0m, in \u001b[0;36mWrapModule.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m-> 1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# Data Collector\n",
    "###############################\n",
    "print(policies['adversary'])\n",
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    TensorDictSequential([policies['adversary'], policies['agent']]),\n",
    "    device=vmas_device,\n",
    "    storing_device=device,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    ")\n",
    "print(collector)\n",
    "\n",
    "###############################\n",
    "# Replay Buffer\n",
    "###############################\n",
    "replay_buffers = {}\n",
    "for group, _agents in env.group_map.items():\n",
    "    replay_buffer = ReplayBuffer(\n",
    "        # TODO: lazytensor or lazymemmap\n",
    "        storage=LazyTensorStorage(frames_per_batch, device=device),  # We will store up to memory_size multi-agent transitions\n",
    "        sampler=SamplerWithoutReplacement(), \n",
    "        batch_size=minibatch_size,  # Sample this size\n",
    "    )\n",
    "    replay_buffers[group] = replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Loss Function\n",
    "###############################\n",
    "losses = {}\n",
    "GAEs = {}\n",
    "optimisers = {}\n",
    "for group, _agents in env.group_map.items():\n",
    "    loss_module = ClipPPOLoss(\n",
    "        actor_network=policies[group],  # Use the non-explorative policies\n",
    "        critic_network=critics[group],\n",
    "        clip_epsilon=clip_epsilon,\n",
    "        entropy_coef=entropy_eps,\n",
    "        normalize_advantage=False,  # Important to avoid normalizing across the agent dimension\n",
    "    )\n",
    "    loss_module.set_keys(\n",
    "        reward=(group, \"reward\"),\n",
    "        action=(group, \"action\"),\n",
    "        sample_log_prob=(group, \"sample_log_prob\"),\n",
    "        value=(group, \"state_value\"),\n",
    "        # These last 2 keys will be expanded to match the reward shape\n",
    "        done=(group, \"done\"),\n",
    "        terminated=(group, \"terminated\"),\n",
    "    )\n",
    "\n",
    "\n",
    "    loss_module.make_value_estimator(\n",
    "        ValueEstimators.GAE, # TD0 VS GAE\n",
    "        gamma=gamma, \n",
    "        lmbda=lmbda\n",
    "    ) \n",
    "    losses[group] = loss_module\n",
    "\n",
    "    GAE = loss_module.value_estimator # we build GAE TODO: do for each agent/adv?\n",
    "    GAEs[group] = GAE\n",
    "\n",
    "    optimisers[group] = torch.optim.Adam(loss_module.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode_reward_mean = 0:   0%|          | 0/10 [00:11<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyncDataCollector(\n",
      "    env=TransformedEnv(\n",
      "        env=VmasEnv(num_envs=60, n_agents=3, batch_size=torch.Size([60]), device=cuda:0) (scenario=simple_tag),\n",
      "        transform=RewardSum(keys=[('adversary', 'reward'), ('agent', 'reward')])),\n",
      "    policy=ProbabilisticActor(\n",
      "        module=ModuleList(\n",
      "          (0): TensorDictModule(\n",
      "              module=Sequential(\n",
      "                (0): MultiAgentMLP(\n",
      "                    MLP(\n",
      "                      (0): Linear(in_features=14, out_features=256, bias=True)\n",
      "                      (1): Tanh()\n",
      "                      (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "                      (3): Tanh()\n",
      "                      (4): Linear(in_features=256, out_features=4, bias=True)\n",
      "                    ),\n",
      "                    n_agents=2,\n",
      "                    share_params=True,\n",
      "                    centralized=False,\n",
      "                    agent_dim=-2)\n",
      "                (1): NormalParamExtractor(\n",
      "                  (scale_mapping): biased_softplus()\n",
      "                )\n",
      "              ),\n",
      "              device=cuda:0,\n",
      "              in_keys=[('adversary', 'observation')],\n",
      "              out_keys=[('adversary', 'loc'), ('adversary', 'scale')])\n",
      "          (1): SafeProbabilisticModule()\n",
      "        ),\n",
      "        device=cuda:0,\n",
      "        in_keys=[('adversary', 'observation')],\n",
      "        out_keys=[('adversary', 'loc'), ('adversary', 'scale'), ('adversary', 'action'), ('adversary', 'sample_log_prob')]),\n",
      "    td_out=TensorDict(\n",
      "        fields={\n",
      "            adversary: TensorDict(\n",
      "                fields={\n",
      "                    action: Tensor(shape=torch.Size([60, 100, 2, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                    episode_reward: Tensor(shape=torch.Size([60, 100, 2, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                    loc: Tensor(shape=torch.Size([60, 100, 2, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                    observation: Tensor(shape=torch.Size([60, 100, 2, 14]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                    sample_log_prob: Tensor(shape=torch.Size([60, 100, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                    scale: Tensor(shape=torch.Size([60, 100, 2, 2]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "                batch_size=torch.Size([60, 100, 2]),\n",
      "                device=cuda:0,\n",
      "                is_shared=True),\n",
      "            agent: TensorDict(\n",
      "                fields={\n",
      "                    action: Tensor(shape=torch.Size([60, 100, 1, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                    episode_reward: Tensor(shape=torch.Size([60, 100, 1, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                    observation: Tensor(shape=torch.Size([60, 100, 1, 12]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "                batch_size=torch.Size([60, 100, 1]),\n",
      "                device=cuda:0,\n",
      "                is_shared=True),\n",
      "            collector: TensorDict(\n",
      "                fields={\n",
      "                    traj_ids: Tensor(shape=torch.Size([60, 100]), device=cuda:0, dtype=torch.int64, is_shared=True)},\n",
      "                batch_size=torch.Size([60, 100]),\n",
      "                device=cuda:0,\n",
      "                is_shared=True),\n",
      "            done: Tensor(shape=torch.Size([60, 100, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "            next: TensorDict(\n",
      "                fields={\n",
      "                    adversary: TensorDict(\n",
      "                        fields={\n",
      "                            episode_reward: Tensor(shape=torch.Size([60, 100, 2, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                            observation: Tensor(shape=torch.Size([60, 100, 2, 14]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                            reward: Tensor(shape=torch.Size([60, 100, 2, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "                        batch_size=torch.Size([60, 100, 2]),\n",
      "                        device=cuda:0,\n",
      "                        is_shared=True),\n",
      "                    agent: TensorDict(\n",
      "                        fields={\n",
      "                            episode_reward: Tensor(shape=torch.Size([60, 100, 1, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                            observation: Tensor(shape=torch.Size([60, 100, 1, 12]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                            reward: Tensor(shape=torch.Size([60, 100, 1, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "                        batch_size=torch.Size([60, 100, 1]),\n",
      "                        device=cuda:0,\n",
      "                        is_shared=True),\n",
      "                    done: Tensor(shape=torch.Size([60, 100, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                    terminated: Tensor(shape=torch.Size([60, 100, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "                batch_size=torch.Size([60, 100]),\n",
      "                device=cuda:0,\n",
      "                is_shared=True),\n",
      "            terminated: Tensor(shape=torch.Size([60, 100, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "        batch_size=torch.Size([60, 100]),\n",
      "        device=cuda:0,\n",
      "        is_shared=True),\n",
      "exploration=random)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\tensordict\\base.py:5028: DeprecationWarning: The entry you have queried with `get` is not present in the tensordict. Currently, this raises an exception. To align with `dict.get`, this behaviour will be changed in v0.7 and a `None` value will be returned instead (no error will be raised). To suppress this warning and use the new behaviour (recommended), call `tensordict.set_get_defaults_to_none(True)` or set the env variable `export TD_GET_DEFAULTS_TO_NONE='1'`. To suppress this warning and keep the old behaviour, call `tensordict.set_get_defaults_to_none(False)` or set the env variable `export TD_GET_DEFAULTS_TO_NONE='0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'key \"action\" not found in TensorDict with keys [\\'episode_reward\\', \\'observation\\']'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11404\\4260170491.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mepisode_reward_mean_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_group_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcollector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'adversary'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mtensordict_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcollector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# tensordict_data.set(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m#     (\"next\", group, \"done\"),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\torchrl\\collectors\\collectors.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensorDictBase\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m         \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\torchrl\\collectors\\collectors.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1031\u001b[0m                 \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_frames\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_frames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1035\u001b[1;33m                 \u001b[0mtensordict_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrollout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1036\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtensordict_out\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1037\u001b[0m                     \u001b[1;31m# if a replay buffer is passed, there is no tensordict_out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                     \u001b[1;31m#  frames are updated within the rollout function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\torchrl\\_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    477\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0munpack_rref_and_invoke_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m         \u001b[1;31m# windows does not know torch._C._distributed_rpc.PyRRef\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_os_is_windows\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distributed_rpc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPyRRef\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m             \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 481\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\torch\\utils\\_contextlib.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\torchrl\\collectors\\collectors.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1162\u001b[0m                         \u001b[1;31m# env_input = self._shuttle.clear_device_()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1163\u001b[0m                         \u001b[0menv_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shuttle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1164\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1165\u001b[0m                     \u001b[0menv_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shuttle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1166\u001b[1;33m                 \u001b[0menv_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_next_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_and_maybe_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shuttle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0menv_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m                     \u001b[1;31m# ad-hoc update shuttle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\torchrl\\envs\\common.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m   2858\u001b[0m                 is_shared=False)\n\u001b[0;32m   2859\u001b[0m         \"\"\"\n\u001b[0;32m   2860\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtensordict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2861\u001b[0m             \u001b[0mtensordict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensordict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2862\u001b[1;33m         \u001b[0mtensordict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensordict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2863\u001b[0m         \u001b[1;31m# done and truncated are in done_keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2864\u001b[0m         \u001b[1;31m# We read if any key is done.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2865\u001b[0m         \u001b[0mtensordict_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_mdp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensordict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\torchrl\\envs\\common.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m   1501\u001b[0m         \u001b[1;31m# sanity check\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1502\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_tensordict_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensordict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mnext_preset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensordict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"next\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1505\u001b[1;33m         \u001b[0mnext_tensordict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensordict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1506\u001b[0m         \u001b[0mnext_tensordict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_proc_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_tensordict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1507\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnext_preset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1508\u001b[0m             \u001b[1;31m# tensordict could already have a \"next\" key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\torchrl\\envs\\transforms\\transforms.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[1;31m# No need to clone here because inv does it already\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m         \u001b[1;31m# tensordict = tensordict.clone(False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m         \u001b[0mnext_preset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensordict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"next\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m         \u001b[0mtensordict_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensordict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 784\u001b[1;33m         \u001b[0mnext_tensordict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensordict_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    785\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnext_preset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;31m# tensordict could already have a \"next\" key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[1;31m# this could be done more efficiently by not excluding but just passing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\torchrl\\envs\\libs\\vmas.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[0magent_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[0maction_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[0mn_agents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_names\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 535\u001b[1;33m             \u001b[0mgroup_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensordict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"action\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m             \u001b[0mgroup_action_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_action\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m             agent_indices.update(\n\u001b[0;32m    538\u001b[0m                 {\n",
      "\u001b[1;32mc:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\tensordict\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, *args, **kwargs)\u001b[0m\n\u001b[0;32m   5033\u001b[0m                     \u001b[1;34m\"To suppress this warning and use the new behaviour (recommended), call `tensordict.set_get_defaults_to_none(True)` or set the env variable `export TD_GET_DEFAULTS_TO_NONE='1'`. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5034\u001b[0m                     \u001b[1;34m\"To suppress this warning and keep the old behaviour, call `tensordict.set_get_defaults_to_none(False)` or set the env variable `export TD_GET_DEFAULTS_TO_NONE='0'`.\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5035\u001b[0m                     \u001b[0mcategory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDeprecationWarning\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5036\u001b[0m                 )\n\u001b[1;32m-> 5037\u001b[1;33m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\tensordict\\_td.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m   2499\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfirst\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2500\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2501\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2502\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2503\u001b[1;33m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2504\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m\"has no attribute\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2505\u001b[0m                 raise ValueError(\n\u001b[0;32m   2506\u001b[0m                     \u001b[1;34mf\"Expected a TensorDictBase instance but got {type(first)} instead\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\tensordict\\_td.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m   2497\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2498\u001b[1;33m         \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2499\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfirst\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2500\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2501\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\tensordict\\_td.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m   2490\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2491\u001b[0m         \u001b[0mfirst_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2492\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensordict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2493\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2494\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2495\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dfc49\\miniconda3\\envs\\simple_tag-gpu\\Lib\\site-packages\\tensordict\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m   4970\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdefault\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNO_DEFAULT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4971\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4972\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4973\u001b[0m             \u001b[1;31m# raise KeyError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4974\u001b[1;33m             raise KeyError(\n\u001b[0m\u001b[0;32m   4975\u001b[0m                 \u001b[0m_KEY_ERROR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4976\u001b[0m             )\n",
      "\u001b[1;31mKeyError\u001b[0m: 'key \"action\" not found in TensorDict with keys [\\'episode_reward\\', \\'observation\\']'"
     ]
    }
   ],
   "source": [
    "# FIX DATA COLLECTOR\n",
    "\n",
    "pbar = tqdm(total=n_iters, desc=\"episode_reward_mean = 0\")\n",
    "\n",
    "episode_reward_mean_list = []\n",
    "for group in train_group_map.keys():\n",
    "    print(collector['adversary'])\n",
    "    for tensordict_data in collector[group]:\n",
    "        pass\n",
    "        # tensordict_data.set(\n",
    "        #     (\"next\", group, \"done\"),\n",
    "        #     tensordict_data.get((\"next\", \"done\"))\n",
    "        #     .unsqueeze(-1)\n",
    "        #     .expand(tensordict_data.get_item_shape((\"next\", (group, \"reward\")))),\n",
    "        # )\n",
    "        # tensordict_data.set(\n",
    "        #     (\"next\", group, \"terminated\"),\n",
    "        #     tensordict_data.get((\"next\", \"terminated\"))\n",
    "        #     .unsqueeze(-1)\n",
    "        #     .expand(tensordict_data.get_item_shape((\"next\", (group, \"reward\")))),\n",
    "        # )\n",
    "        # We need to expand the done and terminated to match the reward shape (this is expected by the value estimator)\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     GAE(\n",
    "        #         tensordict_data,\n",
    "        #         params=loss_module.critic_network_params,\n",
    "        #         target_params=loss_module.target_critic_network_params,\n",
    "        #     )  # Compute GAE and add it to the data\n",
    "\n",
    "        # data_view = tensordict_data.reshape(-1)  # Flatten the batch size to shuffle data\n",
    "        # replay_buffer.extend(data_view)\n",
    "\n",
    "        # for _ in range(num_epochs):\n",
    "        #     for _ in range(frames_per_batch // minibatch_size):\n",
    "        #         subdata = replay_buffer.sample()\n",
    "        #         loss_vals = loss_module(subdata)\n",
    "\n",
    "        #         loss_value = (\n",
    "        #             loss_vals[\"loss_objective\"]\n",
    "        #             + loss_vals[\"loss_critic\"]\n",
    "        #             + loss_vals[\"loss_entropy\"]\n",
    "        #         )\n",
    "\n",
    "        #         loss_value.backward()\n",
    "\n",
    "        #         torch.nn.utils.clip_grad_norm_(\n",
    "        #             loss_module.parameters(), max_grad_norm\n",
    "        #         )  # Optional\n",
    "\n",
    "        #         optim.step()\n",
    "        #         optim.zero_grad()\n",
    "\n",
    "        # collector.update_policy_weights_()\n",
    "\n",
    "        # # Logging\n",
    "        # done = tensordict_data.get((\"next\", group, \"done\"))\n",
    "        # episode_reward_mean = (\n",
    "        #     tensordict_data.get((\"next\", group, \"episode_reward\"))[done].mean().item()\n",
    "        # )\n",
    "        # episode_reward_mean_list.append(episode_reward_mean)\n",
    "        # pbar.set_description(f\"episode_reward_mean = {episode_reward_mean}\", refresh=False)\n",
    "        # pbar.update()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple_tag-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
